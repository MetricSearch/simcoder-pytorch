{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Perfect Point\n",
    "\n",
    "This code runs the perfect point experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:07:14.947759Z",
     "start_time": "2023-05-18T09:07:14.544246Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nasty import hack - this is a code smell, work out how to remove it\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import simcoder.perfect_point_harness as pph\n",
    "import simcoder.average_dist_harness as adh\n",
    "import simcoder.simplex_harness as sph\n",
    "from simcoder.count_cats import findHighlyCategorisedInDataset\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:08:01.089520Z",
     "start_time": "2023-05-18T09:07:14.949565Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# import simcoder.perfect_point_harness as pph # moved below to aid reload\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Load the data:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data_root \u001b[39m=\u001b[39m Path(\u001b[39m\"\u001b[39m\u001b[39m/Volumes/data/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m data \u001b[39m=\u001b[39m load_mf_encodings(data_root \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmf_resnet50\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39m# load resnet 50 encodings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m sm_data \u001b[39m=\u001b[39m load_mf_softmax(data_root \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmf_softmax\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# load the softmax data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoaded datasets\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspace/simcoder-pytorch/experiments/../simcoder/similarity.py:30\u001b[0m, in \u001b[0;36mload_mf_encodings\u001b[0;34m(encodings_dir)\u001b[0m\n\u001b[1;32m     28\u001b[0m paths \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(paths, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m p: \u001b[39mint\u001b[39m(p\u001b[39m.\u001b[39mstem))\n\u001b[1;32m     29\u001b[0m encodings \u001b[39m=\u001b[39m [loadmat(p)[\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m paths]\n\u001b[0;32m---> 30\u001b[0m encodings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(encodings)\n\u001b[1;32m     31\u001b[0m \u001b[39mreturn\u001b[39;00m encodings\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from simcoder.similarity import load_mf_encodings\n",
    "from simcoder.similarity import load_mf_softmax\n",
    "# import simcoder.perfect_point_harness as pph # moved below to aid reload\n",
    "# Load the data:\n",
    "\n",
    "data_root = Path(\"/Volumes/data/\")\n",
    "\n",
    "data = load_mf_encodings(data_root / \"mf_resnet50\") # load resnet 50 encodings\n",
    "sm_data = load_mf_softmax(data_root / \"mf_softmax\") # load the softmax data\n",
    "\n",
    "print(\"Loaded datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-18T09:08:01.091323Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "nn_at_which_k : int = 100\n",
    "number_of_categories_to_test : int = 2\n",
    "threshold = 0.95\n",
    "\n",
    "top_categories,counts = findHighlyCategorisedInDataset(sm_data, threshold)  # get the top categories in the dataset\n",
    "top_categories = top_categories[0: number_of_categories_to_test]  # subset the top categories\n",
    "\n",
    "queries = pph.getQueries(top_categories,sm_data)  # get one query in each category\n",
    "\n",
    "perp_results : pd.DataFrame = pph.run_experiment(queries, top_categories, data, sm_data, threshold, nn_at_which_k ) # TODO check the nn later\n",
    "simp_results : pd.DataFrame = sph.run_experiment(queries, top_categories, data, sm_data, threshold, nn_at_which_k ) # TODO check the nn later\n",
    "aver_results : pd.DataFrame = adh.run_experiment(queries, top_categories, data, sm_data, threshold, nn_at_which_k ) # TODO check the nn later\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "perp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "simp_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aver_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
