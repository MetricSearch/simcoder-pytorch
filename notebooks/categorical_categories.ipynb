{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most highly categorised data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:41:17.240771Z",
     "start_time": "2023-05-17T14:41:15.004044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch torchvision scipy matplotlib | grep -v 'already satisfied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:41:17.244647Z",
     "start_time": "2023-05-17T14:41:17.241419Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "# nasty import hack - this is a code smell, work out how to remove it\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:41:17.247968Z",
     "start_time": "2023-05-17T14:41:17.246369Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_mf_softmax():\n",
    "    basepath = pathlib.Path(\"/Volumes/Data/mf_alexnet_softmax/\")\n",
    "\n",
    "    paths = basepath.glob(\"*.mat\")\n",
    "    paths = sorted(paths, key=lambda p: int(p.stem))\n",
    "    encodings = [loadmat(p)[\"features\"] for p in paths]\n",
    "    encodings = np.concatenate(encodings)\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:41:17.382785Z",
     "start_time": "2023-05-17T14:41:17.249878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File imagenet_classes.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# Download ImageNet labels\n",
    "\n",
    "! if [ -f \"imagenet_classes.txt\" ]; then echo \"File imagenet_classes.txt already exists\"; else wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt 2> /dev/null; fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:41:17.383418Z",
     "start_time": "2023-05-17T14:41:17.369820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:42:21.013786Z",
     "start_time": "2023-05-17T14:41:17.383290Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load the softmax encodings of the data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m smData \u001b[39m=\u001b[39m load_mf_softmax()\n\u001b[1;32m      5\u001b[0m \u001b[39m# Load the data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpathlib\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mload_mf_softmax\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m paths \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(paths, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m p: \u001b[39mint\u001b[39m(p\u001b[39m.\u001b[39mstem))\n\u001b[1;32m      6\u001b[0m encodings \u001b[39m=\u001b[39m [loadmat(p)[\u001b[39m\"\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m paths]\n\u001b[0;32m----> 7\u001b[0m encodings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(encodings)\n\u001b[1;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m encodings\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# Load the softmax encodings of the data\n",
    "\n",
    "smData = load_mf_softmax()\n",
    "\n",
    "# Load the data\n",
    "\n",
    "import pathlib\n",
    "from simcoder.similarity import load_mf_encodings\n",
    "\n",
    "allData = load_mf_encodings(pathlib.Path(\"/Volumes/Data/mf_resnet50\")) # load resnet 50 encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:44:59.570370Z",
     "start_time": "2023-05-17T14:44:57.531839Z"
    }
   },
   "outputs": [],
   "source": [
    "from simcoder.count_cats import findHighlyCategorisedInDataset\n",
    "from simcoder.count_cats import countNumberinCatGTThresh\n",
    "\n",
    "threshold = 0.95\n",
    "\n",
    "top_cats, counts  = findHighlyCategorisedInDataset(smData,threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T14:45:00.777460Z",
     "start_time": "2023-05-17T14:45:00.732408Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    print( top_cats[i], counts[i], categories[i], countNumberinCatGTThresh(top_cats[i],threshold,smData) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from simcoder.experiment import get_nth_categorical_query\n",
    "\n",
    "\n",
    "top_cats = top_cats[0: 100]  # subset the top categories\n",
    "queries = get_nth_categorical_query(top_cats,smData,20)  # get one query in each category\n",
    "\n",
    "from simcoder.similarity import get_mf_image\n",
    "\n",
    "get_mf_image(queries[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
